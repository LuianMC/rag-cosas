{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ZgjwK_LarG"
      },
      "source": [
        "# RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "---\n",
        "\n",
        "RAG en palabras simples es darle información al modelo para aumentar su ventana de contexto y que pueda responder preguntas especificas acerca de esos datos o información adicional.\n",
        "[Aquí está la informacion sobre RAG](https://python.langchain.com/docs/use_cases/question_answering/).\n",
        "Es recomendable que al comenzar con RAG se utilice LangChain, ya que es fácil de usar y bastante intuitivo.\n",
        "\n",
        "¿Cómo le damos contexto a nuestro modelo? Bueno, primero utilizamos dos cosas, [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?hl=es-419) y [VectorStores](https://python.langchain.com/docs/modules/data_connection/vectorstores/). **Leer esa documentación**.\n",
        "\n",
        "Los pasos a seguir son los siguientes:\n",
        "\n",
        "*   Cargo mis datos o documentos.\n",
        "*   Parto estos documentos o datos en trozos.\n",
        "*   Utilizo un modelo de Embeddings para crear Embeddings con mis documentos o datos.\n",
        "*   Guardo estos Embeddings en una base de datos (VectorStore).\n",
        "*   Cargo mi modelo.\n",
        "*   Inicializo mi base de datos para que pueda servir para obtener información de ella\n",
        "*   Indico que voy a utilizar esa base de datos para obtener la información.\n",
        "*   Hago mi pregunta.\n",
        "*   La pregunta pasa primero por la base de datos.\n",
        "*   La base de datos recupera toda la informacion similar a la pregunta realizada.\n",
        "*   Luego tanto el contexto como la pregunta es enviada al modelo para que haga la inferencia.\n",
        "\n",
        "---\n",
        "\n",
        "Recomiendo mucho leer la documentación sobre RAG de LangChain porque lo explica muy bien y tiene material visual con diagramas para entenderlo más fácil. También, para entender que es lo que pasa con tu información, te recomiendo leer la documentación sobre Embeddings y VectorStores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHXU6mcsG-l6"
      },
      "source": [
        "# Requisitos\n",
        "---\n",
        "Para poder utilizar RAG en LangChain debes tener los siguientes paquetes de Python instalados.\n",
        "\n",
        "*   `langchain`\n",
        "*   `openai` (en el caso de que uses modelos como GPT)\n",
        "*   `chromadb` (es la base de datos (VectorStore) donde se guardaran tus Embeddings)\n",
        "*   `tiktoken`\n",
        "*   `pypdf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rLn1zbAvIT3A"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\" # Aquí pon tu API key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfedwOkf6Kq4",
        "outputId": "c2d98121-54b6-4405-c526-ed91e1f1e288"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# Se carga el contenido de un PDF y se transforma a un archivo de texto.\n",
        "# Aquí se utiliza un solo documento, pero se pueden usar más documentos.\n",
        "loader = PyPDFLoader(\n",
        "    \"./docs/2018-DECRETO-REGLAMENTO-GENERAL-DE-DOCENCIA-DE-PREGRADO.pdf\"\n",
        ") # Fuente: https://docencia.udec.cl/documentos/reglamentos-de-pregrado/\n",
        "\n",
        "# loader = UnstructuredCSVLoader(\"/content/Reporte_Sensores_Maquinas_Industriales.csv\", mode=\"single\")\n",
        "\n",
        "# Esta línea carga los documentos como una variable para ser utilizada más adelante.\n",
        "docs = loader.load()\n",
        "\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRAqiJow60yY",
        "outputId": "56479473-894e-4f2d-d9fe-c4e2d9821e4b"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Esta línea se encarga de partir el texto en trozos de 1200 Chars (caracteres) con un Overlap (superpisición) de 0.\n",
        "# Lo ideal es tener un Overlap un poco más grande, pero hay que ir viendo cual sería el óptimo.\n",
        "# La razón de partir el texto en trozos es principalmente facilitar el hacer Embeddings con el texto.\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 1200, chunk_overlap = 0)\n",
        "\n",
        "# Aquí indicamos que queremos partir los documentos y guardarlos en una variable.\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_q3buIpZ9hsQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Aquí indicamos que queremos utilizar como modelo de Embedding text-embedding-ada-002,\n",
        "# recomendado por openAI, pero puedes utilizar uno local (open source) de Huggingface.\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\" # Modelos de OpenAI y sus precios: https://openai.com/api/pricing/\n",
        "\n",
        "# Aquí creamos un VestorStore, para almacenar los Embeddings en índices y facilitar la recuperación de los datos.\n",
        "# Se indican los documentos que vas a pasar a Embeddings, aquí utilizo OpenAI (como modelo de Embedding),\n",
        "# pero se puede cambiar, el nombre de la colección, no es relevante a menos que tengas varios VectorStores.\n",
        "vectorstore = Chroma.from_documents(documents = all_splits, embedding = OpenAIEmbeddings(model = EMBEDDING_MODEL), collection_name = \"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK5wmQRYhxyt",
        "outputId": "c41d4414-98c1-4341-f676-ab9d9da9fc83"
      },
      "outputs": [],
      "source": [
        "## TEST\n",
        "\n",
        "question = \"Plan de Estudio\"\n",
        "\n",
        "# Aquí estoy testeando si me devuelve todo el texto que encuentre similar o que contenga \"Plan de estudio\".\n",
        "docs = vectorstore.similarity_search(question)\n",
        "\n",
        "# len(docs)\n",
        "\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAHDeZzPimjV",
        "outputId": "d6c1f6be-8a15-4dfb-f622-b5c98f60dc76"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# Aquí traemos un modelo de OpenAI, en este caso gpt-3.5-turbo. La temperatura indica la creatividad del modelo.\n",
        "# Cuando quieres que te devuelve la información lo más parecida debes usar temperatura 0,\n",
        "# para que sea más creativo se debe usar temperatura 1.\n",
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0)\n",
        "\n",
        "# Aquí cargamos la Prompt que envuelve tanto el contexto como tu pregunta.\n",
        "# Esto es para indicarle al modelo como debe actuar una vez recibe todo.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\") # Fuente: https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "\n",
        "# Esto es para formatear los documentos.\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Aquí creamos la cadena de RAG, primero pasamos la base de datos como un Retriever, para obtener de ahí la información,\n",
        "# luego indicamos como va a recibir la pregunta.\n",
        "# Pasamos la Prompt, el modelo y un Parser (analizador) para la respuesta.\n",
        "rag_chain = (\n",
        "    {\"context\": vectorstore.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"tu pregunta aquí\") # reemplazar"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
